---
title: "Bayesian computational mixed models in R"
author: Ben Bolker
bibliography: "../glmm.bib"
date: "`r format(Sys.time(), '%d %B %Y ')`"
---

## Basic Bayes

Posterior probability = likelihood $\times$ prior

```{r bayesplot1,echo=FALSE}
data("ReedfrogPred",package="emdbook")
x = subset(ReedfrogPred,pred=="pred" & density==10 & size=="small")
k = x$surv
op=par(lwd=2,bty="l",las=1,cex=1.5)
curve(dbeta(x,shape1=sum(k)+1,shape2=40-sum(k)+1),
      xlab="Predation probability\nper capita",
      ylab="Probability density",
      from=0,to=1,ylim=c(0,13))
curve(dbeta(x,shape1=sum(k),shape2=40-sum(k)),
      add=TRUE,lwd=3)
curve(dbeta(x,shape1=1,shape2=1),col="darkgray",add=TRUE,
      type="s")
curve(dbeta(x,shape1=121,shape2=81),
      add=TRUE,lty=2,col="darkgray")
curve(dbeta(x,shape1=151,shape2=91),
      add=TRUE,lty=2,n=200)
tlocs <- list(x=c(0.44,0.13,0.82,0.75,0.95),
              y=c(10,3.1,10.8,8,5.9))
text(tlocs$x,tlocs$y,c("prior\n(121,81)",
                       "prior\n(1,1)",
                       "posterior\n(151,111)",
                       "posterior\n(31,11)",
                       "scaled\nlikelihood"),
     cex=0.6)
alocs <- list(x=c(0.151,0.464,0.734,0.720,0.924,
                  0.18, 0.547,0.656,0.725,0.843),
              y=c(2.3,9.047,10.806,7.241,4.833,
                  1.02,7.195,9.973,5.898,3.212))
arrows(alocs$x[1:5],alocs$y[1:5],alocs$x[6:10],alocs$y[6:10],
       angle=15,col=rep(c("darkgray","black"),c(2,3)))
par(op)
```

```{r bayes_prior_unif,echo=FALSE}
op <- par(cex=1.5,las=1,bty="l",lwd=2,yaxs="i")
gcols <- gray(c(0.2,0.8))
b1 <- barplot(t(matrix(c(1/3,1/3,1/3,1/4,1/4,1/2),ncol=2)),beside=TRUE,
              xlab="Predator",ylab="Probability",space=c(0.2,2),
              col=gcols,yaxs="i")
axis(side=1,at=colMeans(b1),c("raccoon","squirrel","snake"))
segments(b1[1,1],0.4,b1[2,2],0.4)
text((b1[1,1]+b1[2,2])/2,0.45,"mammalian")
par(xpd=NA)
legend(2,0.57,c("by species","by group"),
       ncol=2,fill=gcols,bty="n")
par(op)
```

```{r bayes_prior_unif_cont,echo=FALSE}
minx <- 10
maxx <- 100
dx <- maxx-minx
dlx <- log(maxx/minx)
dlx10 <- log10(maxx/minx)
xlim <- c(0,110)
Lxlim <- c(9,110)
op <- par(cex=2,las=1,bty="l",lwd=2,mfrow=c(1,2),
          mar=c(5,4,3,0.5)+0.1,yaxs="i")
curve(ifelse(x>minx & x<maxx,1/dx,0),from=xlim[1],to=xlim[2],
      xlab="Mass",ylab="Probability density",ylim=c(0,0.04),main="linear scale",type="s",
      cex.main=1.5,xlim=c(0,100),n=400,axes=FALSE)
axis(side=1,at=c(10,100))
axis(side=2,at=c(0,0.02,0.04))
box()
curve(ifelse(x>minx & x<maxx,1/x*1/dlx,0),from=xlim[1],to=xlim[2],
      lty=2,n=400,add=TRUE)
legend("topright",c("uniform","log-uniform"),lty=1:2)
curve(ifelse(x>log(minx) & x<log(maxx),1/dlx,0),
      from=log(Lxlim[1]),to=log(Lxlim[2]),
            ylim=c(0,1.2),
      xlab="Log mass",ylab="",axes=FALSE,lty=2,main="log scale",type="s",
      cex.main=1.5,n=400)
curve(ifelse(x>log(minx) & x<log(maxx),exp(x)/dx,0),from=log(Lxlim[1]),to=log(Lxlim[2]),
      add=TRUE,n=400)
axis(side=1,
     at=log(c(10,100)),
     labels = paste("log(",c(10,100),")",sep=""))
axis(side=2,at=c(0,0.5,1))
box()
par(op)
```

## Priors, continued

- debate over best way to use priors continues
- "uninformative" or "flat": probably not really
- "conjugate": mathematically/computationally convenient
- **regularizing** priors: neutral, but not completely uninformative
     - keeps model from misbehaving
	 - **informative** priors: non-neutral, real information [@Crome+1996,@McCarthy2007]
- simple 'uninformative' priors for variances etc. might not be [@gelman_prior_2006]
- uniform priors are simple but maybe problematic [@carpenter_computational_2017]

```{r echo=FALSE,eval=FALSE}
## faffing around with priors peaked near zero
## invgamma is unstable/I don't really understand it properly
f1 <- function(x,a=1.35) x*dt(x/a,df=3)
f2 <- function(x) MCMCpack::dinvgamma(x,shape=1.05,scale=0.05)
f3 <- function(x) dgamma(x,shape=1/20,scale=20)
curve(f2,n=501)
curve(f3,add=TRUE,col=2)
mean(rinvgamma(1000000,shape=3,scale=1))
i1 <- function(m,u=100) { integrate(m,lower=0, upper=u) }
i2 <- function(m,u=100) { integrate(function(x) m(x)*x, lower=0, upper=u) }
i1(f2)
i2(f2)
## mean = scale/(shape-1)
## -> scale = (shape-1)*mean for alpha>1
## https://www.johndcook.com/inverse_gamma.pdf
## 
```

## typical neutral/regularizing priors

- fixed-effect parameters  [@greenland_penalization_2015]
    - typically Normal, mean 0, std dev 3--5 
    - assume parameters are scaled or log/logit scale
	- $t$/Cauchy allow heavier tails, but Normal is easier
- variance parameters
    - Gamma(small shape): typical but problematic
    - Gamma(2): weakly regularizing
- correlation matrices
    - Wishart, inverse-Wishart: small shape parameters
    - LKJ priors [@lewandowski_generating_2009]: `eta>1` makes extreme correlations less likely

## Sampling

- in prin
## Metropolis-Hastings
