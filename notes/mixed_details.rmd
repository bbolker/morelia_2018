---
title: "Linear mixed models in R: nitty-gritty"
author: Ben Bolker
bibliography: "../glmm.bib"
date: "`r format(Sys.time(), '%d %B %Y ')`"
---

```{r pkgs,echo=FALSE,message=FALSE}
library(ggplot2);theme_set(theme_bw())
library(ggExtra)
library(broom)
library(cowplot)
```

# Model specification

## Distribution/family

- going to assume Gaussian (Normal) here
- may need to transform
- we care about the **conditional** distribution, not the marginal
- Box-Cox not implemented: could try `MASS::boxcox()` on residuals
- brute-force: if it makes sense or *if* residuals look bad, try log/sqrt?
- bias $\gg$ heteroscedasticity/outliers $\gg$ Normality

---

```{r marg_plot,echo=FALSE,fig.width=10}
set.seed(101)
dd <- data.frame(x=runif(300,0,500))
dd$y <- with(dd,rnorm(300,mean=1+2*x,sd=10))
lm1 <- lm(y~x,data=dd)
g1 <- ggMarginal(ggplot(dd,aes(x,y))+geom_point()+
                 geom_smooth(method="lm")+
                 ggtitle("marginal"),
                 margins="y",type="histogram")
aa <- broom::augment(lm1,data=dd)
g2 <- ggExtra::ggMarginal(ggplot(aa,aes(x,.resid))+geom_point()+
                 ggtitle("conditional"),
                 margins="y",type="histogram")
cowplot::plot_grid(g1,g2)
```

## Look at your data!

- use `table()` to confirm how many observations per level of factor
    - which factors covary?
    - balanced?
- plot the data
	
## Random effects: reminder [@bolker_glmm_2014]

- don’t want to test differences across specific groups
- lots of groups (>5), few observations per group, unbalanced
- exchangeable groups

## Formulas

- random effects specified with `|` as `(a|g1) + (b|g2) + ...`
- right-hand side is the *grouping variable* (always categorical, usually a factor)
- left-hand side is the *varying term* (most often 1)s
- terms separated by `+` are independent

see also: http://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet?lq=1 (Livius)

-----------------------------|---------------------------------
 $β_0 + β_{1}X_{i} + e_{si}$ | n/a (Not a mixed-effects model) 
 $β_0 + S_{0s} + β_{1}X_{i} + e_{si} $  | `∼ X+(1∣Subject)`  
 $β_0 + S_{0s} +  (β_{1} + S_{1s})Xi + e_{si}$ | `~ X+(1 + X∣Subject)`
 $β_0 + S_{0s} + I_{0i} +  (β_{1} + S_{1s})Xi + e_{si}$ | `∼ X+(1 + X∣Subject)+(1∣Item)`
 $ β_0 + S_{0s} + I_{0i} + β_{1}X_{i} + e_{si}$  | `∼ X+(1∣Subject)+(1∣Item)` 
 As above, but $S_{0s}$, $S_{1s}$ independent | `∼ X+(1∣Subject)+(0 + X∣ Subject)+(1∣Item)` 
 $β_0 + I_{0i} +  (β_{1} + S_{1s})Xi + e_{si}$ |  `∼ X+(0 + X∣Subject)+(1∣Item)`
 ---------------------------|--------------------------
 
## Formulas, interactions, nesting etc.

`a*b` = main effects plus interaction, `a:b` = interaction only, `a/b` = `a` + `a:b`

- Nested: `(1|f/g)` equivalent to `(1|f) + (1|f:g)`. e.g. subplots vary within plots (but "subplot 1 of every plot" isn't meaningful)
- Crossed: `(1|f) + (1|g)`. e.g. years vary, and plots vary independently
- Crossed+: `(1|f) + (1|g) + (1|f:g)`. e.g. years vary, and plots vary independently, and plots also vary *within* years (for LMMs, assumes >1 observation per plot/year combination). (`(1|f*g)` should be allowed but ...)

Don't need explicit nesting if your sub-groups are uniquely labeled (i.e. `A1`, `A2`, ..., `B1`, `B2`, ...)

## What is the maximal model?

- Which effects vary *within* which groups?
- If effects don't vary within groups, then we *can't* estimate among-group variation in the effect
     - convenient
     - maybe less powerful (among-group variation is lumped into residual variation)
- e.g. female rats exposed to different doses of radiation, multiple pups per mother, multiple measurements per pup (labeled by time). Maximal model ... ?

Maximal model **often won't work**

## Random-slopes models: what does `(x|g)` really do?

- equivalent to `(1+x|g)`
- both intercept (baseline) and slope vary across groups
- estimates **bivariate** zero-centered distribution:

$$
(\textrm{intercept}, \textrm{slope}) =
\textrm{MVN}\left(\boldsymbol 0,
\left[
\begin{array}{cc}
\sigma^2_{\textrm{int}} &
\sigma_{\textrm{int},\textrm{slope}} \\
\sigma_{\textrm{int},\textrm{slope}} & 
\sigma^2_{\textrm{slope}}
\right]
\right)
$$

---

##

- maximal model can get very complicated (e.g. `(1+x+y+z+...|g)`: $n$ effects require $n(n+1)/2$ variances + covariances

## Other FAQs

- Can you have continuous variables as REs?  
(yes, as *varying terms*, but not as *grouping variables*
- Can a variable be in the model as both FE and RE?  
(only in the special case where $x$ is numeric but discrete (e.g. year) and >1 observation per $x$ value;  
FE describes overall trend, RE describes variation around the trend

## Simplified versions of models

- `(1|a/b)` ([positive] compound symmetry) vs. `(a|b)`
- `(1+x+y+z||b)`
     - independent terms
     - expands to `(1|b) + (0+x|b) + ...`
	 - `lme4` version **only works properly for continuous predictors**
	 - `afex::mixed` can do this
	 - independent model is no longer invariant to shifts/reparameterization
- RE "nested within" FE, e.g. if not enough groups at the top level;
`y ~ g1 + (1|g1:g2)`	 
	 
## What is a practical model?

- Fits aren't **singular**
- Zero variances, +/- 1 correlations
- More subtle for larger models:  
`all(abs(getME(x,"theta"))>1e-4)`

## Why are fits singular?

```{r singsims,echo=FALSE,fig.width=7,fig.height=4,cache=TRUE}
simfun <- function(n1=5,n2=5,sd1=1,sd2=1) {
  d <- expand.grid(f1=factor(seq(n1)),f2=factor(seq(n2)))
  u1 <- rnorm(n1,sd=sd1)
  d$y <- rnorm(n1*n2,mean=u1,sd=sd2)
  d
}
require(lme4)
fitfun <- function(d=simfun()) {
  sqrt(unlist(VarCorr(lmer(y~(1|f1),data=d))))
}
set.seed(101)
sd_dist1 <- replicate(500,fitfun())
sd_dist2 <- replicate(500,fitfun(simfun(n1=3)))
sd_List <- list(n1.5=sd_dist1,n1.3=sd_dist2)
plotfun <- function(x,trueval) {
  par(las=1,bty="l")
  hist(x,breaks=50,col="gray",main="",xlab="est. sd",
       freq=FALSE)
}
par(mfrow=c(1,2))
invisible(lapply(sd_List,plotfun))
```

# Inference

## Likelihood

- probability of data given model
- for mixed models, includes the integral over the random effects

## Profiles and intervals

## Wald approximation

most important:

- small sample sizes
- values near boundaries
     - changing scales may help

more important for variance parameters than fixed-effect parameters

## Finite-size corrections

e.g. Normal vs. $t$

## Fit model

## Diagnostics: residual scale

Similar to generalized linear models: fitted vs. residual,
scale-location, Q-Q 

## Posterior predictive simulation

- if there is some value you're interested in that you can compute from your data (e.g. number of zero observations, or total NPP)

## Inference: fixed effects, Wald

- parameter estimates: `summary()`
- termwise tests: `car::Anova`, `afex::anova`
- contrasts/effects/post-hoc tests: `emmeans`, `effects` packages

## Degrees of freedom

- level-counting: `R/calcDenDF.R`
- `lmerTest`/`afex`; Satterthwaite, Kenward-Roger

## Inference: fixed effects, likelihood ratio test

- individual parameters: `profile()`, `confint()`
- fit pairwise models and use `anova()` to compare
- `drop1`, `afex::anova()`

## Inference: parametric bootstrap

- `pbkrtest`
- `confint(.,method="boot")`

## Inference: random effects

- Wald is probably very bad
- profile, `anova()`
- parametric bootstrap
- boundary problems

## Inference: CIs on predictions etc.

```{r eval=FALSE}
mm <- model.matrix(terms(fitted_model),newdat)
se_pred <- sqrt(diag(mm %*% tcrossprod(vcov(fitted_model),mm)))
```
