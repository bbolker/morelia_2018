---
title: Data visualization, focusing on ggplot and multilevel data
author: Ben Bolker
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
bibliography: ../vis.bib
output: html_document
---

![cc](pix/cc-attrib-nc.png)
Licensed under the 
[Creative Commons attribution-noncommercial license](http://creativecommons.org/licenses/by-nc/3.0/).
Please share \& remix noncommercially, mentioning its origin.

```{r setup, echo=FALSE,message=FALSE}
library(knitr)
opts_knit$set(fig.align="center",fig.width=7,fig.height=5,
               out.width="0.7\\textwidth",tidy=FALSE,use.highlight=TRUE)
knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       par(bty="l",las=1)
                   } else { }
               })
library("ggplot2"); theme_set(theme_bw())
``` 

## Goals/contexts of data visualization

**Exploration**

* want *nonparametric*/*robust* approaches: impose as few assumptions as possible
 * boxplots instead of mean/standard deviation (generally base locations on medians rather than means)
 * loess/GAM instead of linear/polynomial regression
* need *speed*: quick and dirty
* canned routines for standard tasks, flexibility for non-standard tasks
* manipulation in the context of visualization: need to summarize on the fly

**Diagnostics**

* attempt to determine fitting problems graphically: looking for absence of patterns in residuals
* e.g. scale-location plot, Q-Q plot; `plot.lm`
* plot methods: generic (e.g. residuals vs fitted) vs specific (e.g. residuals vs predictors)
* plotting predictions (intuitive) vs plotting residuals (amplifies/zooms in on discrepancies)
* plotting unmodeled characteristics (e.g. spatial, temporal autocorrelation): much easier to draw a picture than fit a model
* code contrasts for visual simplicity (e.g. deviations from linearity: Q-Q plots, signed square-root profiles)


**Presentation**

* how closely should one match analyses with graphs?  "Let the data speak for themselves" vs "Tell a story"
* display data (e.g. boxplots, standard deviations) or inferences from data (confidence intervals)
* superimposing model fits (`geom_smooth`)
* avoid excessive cleverness/data density
* coefficient plots vs parameter tables [@gelman_lets_2002]
* tradeoff between visual design (tweaking) and reproducibility: learning to futz with label positioning etc. may pay off in the long run (a few tools exist for automatic placement)
* order factors in a sensible order (i.e *not* alphabetical or numerical unless (1) the labels have some intrinsic meaning or (2) you expect that readers will be interested in looking up particular levels in the plot).  This is sometimes called the "what's so special about Alabama?" problem, although the Canadian version would substitute "Alberta".

## Basic criteria for data presentation

Much of what I have to say here is also said very nicely by @rauser_how_2016

Visual perception of quantitative information: [Cleveland hierarchy](http://processtrends.com/toc_data_visualization.htm) [@cleveland_graphical_1984,@cleveland_graphical_1987,@cleveland_visualizing_1993]

![cleveland](pix/data_vis_1.png)

**Data presentation scales with data size**

* **small** show all points, possibly dodged/jittered, with some summary statistics: dotplot, beeswarm. Simple trends (linear/GLM)
* **medium** boxplots, loess, histograms, GAM (or linear regression)
* **large** modern nonparametrics: violin plots, hexbin plots, kernel densities: computational burden, and display overlapping problems, relevant
* combinations or overlays where appropriate (beanplot)

## Rules of thumb

* (Continuous) response on the $y$-axis, most salient (continuous) predictor on the $x$-axis
* Put most salient comparisons within the same subplot (distinguished by color/shape), and nearby within the subplot when grouping bars/points
* Facet rows > facet columns
* Use transparency to include important but potentially distracting detail
* Do category levels need to be *identified* or just *distinguished*?
* Order categorical variables meaningfully
* Choose *population variation* (standard deviations, boxplots) vs. *estimate variation* (standard errors, mean $\pm$ 2 SE, boxplot notches)
* Try to match graphics to statistical analysis, but not at all costs
* Choose colors carefully (`RColorBrewer`/[ColorBrewer](colorbrewer2.org/), [IWantHue](http://tools.medialab.sciences-po.fr/iwanthue/)): respect dichromats and B&W printouts

## Techniques for multilevel data

* faceting (= trellis plots = small multiples) vs grouping ("spaghetti plots")
* join data within a group by lines (perhaps thin/transparent)
* can colour lines by group (get legend), but more useful for explanatory than presentation graphics

```{r ggplot_mult,fig.keep="none",message=FALSE}
data("cbpp",package="lme4")
## make period *numeric* so lines will be connected/grouping won't happen
cbpp2 <- transform(cbpp,period=as.numeric(as.character(period)))
g0 <- ggplot(cbpp2,aes(period,incidence/size))
## spaghetti plot
g1 <- g0+geom_line(aes(colour=herd))+geom_point(aes(size=size,colour=herd))
g2 <- ggplot(cbpp2,aes(period,incidence/size,colour=herd))
(g3 <- g2 + geom_line()+geom_point(aes(size=size)))
## facet instead
(g4 <- g1+facet_wrap(~herd))
## order by average prop. incidence
g1 %+% transform(cbpp2,herd=reorder(herd,incidence/size))
g4 %+% transform(cbpp2,herd=reorder(herd,incidence/size))
## also consider colouring by incidence/order ...
```

Makes it fairly easy to do a simple *two-stage* analysis on the fly:
```{r ggplot_mult2,fig.keep="none",warning=FALSE}
g0+geom_point(aes(size=size,colour=herd))+
    geom_smooth(aes(colour=herd,weight=size),
                method="glm",
                method.args=list(family=binomial),
                se=FALSE)
```
(ignore `glm.fit` warnings if you try this)


## Challenges

## high-dimensional data (esp continuous)

Possible solutions:

- use color, shape for discrete predictor variables (up to ~10 categories); text plots
- small multiples, conditioning plots (shingles/facets); i.e. discretize continuous plots
- contour plots (worse)
- perspective plots (worst?)

##  large data sets

- problems with computation, file size, presentation
- file size: raster (PNG) instead of vector (PS/PDF), `pch="."`
- overplotting (alpha), kernel density estimation, hexagonal binning
- summarize (quantiles, kernel densities, etc.)

## discrete data

- lots of point overlap; jittering OK for exploratory analysis but ugly.  Need to summarize/bin appropriately (`stat_sum`); beeswarm plots

## spatial data

- the best parts of the Cleveland hierarchy ($x$ and $y$ axes) are already taken, usually have to resort to color/size/pie charts. Representing uncertainty is a big challenge, usually must be done separately (transparency/saturation?)

## compositional data

- would like to display "sum to 1.0" constraint but also allow accurate comparison of magnitudes: stacked bars vs grouped bars (or dotplots)?
- harder if also need to represent uncertainty (would like to show correlations among components)
- *ternary diagrams*: nice but don't generalize past 3 elements

## multilevel data

- often hard (or messy) to represent all levels of variation

## next generation tools 

- dynamic/exploratory graphics: [ggobi](http://www.ggobi.org), [Mondrian](http://rosuda.org/mondrian/Mondrian.html), [latticist] (http://code.google.com/p/latticist), JMP, [Shiny](http://www.rstudio.com/shiny/), [Gapminder](http://www.gapminder.org/), [googleVis](http://code.google.com/p/google-motion-charts-with-r/), [rCharts](http://rcharts.io/), [ggviz](https://github.com/rstudio/ggvis) (next-generation/interactive `ggplot2`)
- GUI frameworks: JMP, [R Commander](http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/), [Deducer](cran.r-project.org/web/packages/Deducer/index.html), [Rattle](rattle.togaware.com/), [web interface to ggplot2](http://www.yeroon.net/ggplot2/), Shiny
- presentation technologies: [JCGS editorial](http://pubs.amstat.org/doi/pdfplus/10.1198/jcgs.2010.191ed) with
[supplementary materials](http://pubs.amstat.org/doi/suppl/10.1198/jcgs.2010.191ed)
- computational frameworks: `lattice`, `ggplot`, [http://d3js.org/](http://vis.stanford.edu/protovis/), `ggviz`

## Graphics culture 

- the gods: Cleveland (hierarchy), Tufte [@tufte_visual_2001;@tufte_envisioning_1995;@tufte_visual_1997;@tufte_beautiful_2006] ("chartjunk", minimizing non-data-ink, sparklines), Tukey
- the demi-gods: Wilkinson (Grammar of Graphics) [@Wilkinson1999], Wickham (ggplot *et al*) [@wickham_ggplot2_2009], [Stephen Few](http://www.perceptualedge.com/), Kaiser Fung ([Junk Charts](http://junkcharts.typepad.com/junk_charts/))
- dionysians (infovis) vs. apollonians (statistical graphics) [@gelman_infovis_2013]
- graphics pedants: [dynamite plots](http://emdbolker.wikidot.com/blog:dynamite), pie charts (esp. 3D), dual-axis figures ...

## Data visualization in R

## Base graphics 

- simple 'canvas' approach
- straightforward, easy to customize
- most plot methods written in base graphics

## ggplot 

- newest
- based on Wilkinson's "Grammar of Graphics"
- documented in a book (see below) and on a [web site](http://had.co.nz/ggplot2), as well as an active [mailing list](http://groups.google.com/group/ggplot2)
- explicit mapping from variables to "aesthetics": e.g. x, y, colour, size, shape
- implements faceting
- some data summaries etc. built in
- easier to overlay multiple data sets, data summaries, model predictions etc.
- no 3D plots (although see the [gg3D package](https://github.com/AckerDWM/gg3D))
- rendering can be slow
- `gridExtra`, `ggExtra`, `cowplot`, `directlabels` packages may be handy
- [ggplot gallery](http://www.ggplot2-exts.org/gallery/)

## ggplot intro

mappings + geoms

## Data

Specified explicitly as part of a `ggplot` call:

```{r ggplot1,message=FALSE,fig.width=4,fig.height=4}
library(mlmRev)
head(Oxboys)
library(ggplot2)
ggplot(Oxboys)
```

But that isn't quite enough: we need to specify a *mapping* between variables (columns in the data set) and *aesthetics* (elements of the graphical display: x-location, y-location, colour, size, shape ...)

```{r ggplot2,fig.width=4,fig.height=4}
ggplot(Oxboys,aes(x=age,y=height))
```

but (as you can see) that's still not quite enough.  We need to specify some geometric objects (called `geom`s) such as points, lines, etc., that will embody these aesthetics.  The weirdest thing about `ggplot` syntax is that these `geom`s get *added* to the existing `ggplot` object that specifies the data and aesthetics; unless you explicitly specify other aesthetics, they are inherited from the initial `ggplot` call.

```{r ggplot3,fig.width=4,fig.height=4}
ggplot(Oxboys,aes(x=age,y=height))+geom_point()
```

- many more geoms (lines, bars, etc.)
- summarizers: smooth lines and summaries (`geom_smooth`, `stat_sum`)
- control of scales (e.g. log transforms, colors, etc.)
- faceting (grid and wrap)

See [Karthik Ram's ggplot intro](https://github.com/karthikram/ggplot-lecture) or [my intro for disease ecologists](http://ms.mcmaster.ca/~bolker/eeid/ecology/ggplot.pdf), among many others.

```{r sessioninfo}
sessionInfo()
```

## References
